{
    "contents" : "# we load the data into R using the .csv file with headers\nNmin<-read.csv(\"stepwise regression data.csv\", header=T)\nnames(Nmin)\nstr(Nmin)\n# we generate our model\nsm1 <- lm( NetNmin28~  WEON+WEOC +WECN + SOM +TSN +TSC  +TSCN+SolvitaCO2+SHI , Nmin)\nsummary(sm1)\nanova(sm1)\nlibrary(car)\nAnova(sm1,type=\"II\")\nAnova(sm1,type=\"III\")\n\nshapiro.test(residuals(sm1)) # Null hypothesis: \"the samples come from a MV Normal distribution\" against the Alternative hypothesis: \"the samples do not come from a MV Normal distribution\"\nqqPlot(sm1, main=\"QQ Plot\") #qq plot for studentized resid\navPlots(sm1,terms=~.)\n#Q2:Testing homogeneity of variance:\n##By location\nNmin$location<-factor(Nmin$location)\nclass(Nmin$location)\nlibrary(HH)\nhov(residuals(sm1)~ location, Nmin)\nsummary(sm1)\nresidualPlots(sm1)\nlayout(matrix(c(1,2,3,4),2,2)) # optional 4 graphs/page\n## by Type\nNmin$location<-factor(Nmin$Type)\nclass(Nmin$Type)\nlibrary(HH)\nhov(residuals(sm1)~ Type, Nmin)\nsummary(sm1)\nresidualPlots(sm1)\nlayout(matrix(c(1,2,3,4),2,2)) \n\n\n#Q3: Testing independence of errors\ndwt(sm1)\n\n#Q4: Testing for liniarity \nleveragePlots(sm1) #leverage plots\n\n#Q5 finding Outliers \n\n#Using the hats values\n### citical value =2 *(p/n)  p=number of parameter (number of independent variables +1) n=number of observation\ndim(Nmin)\n2*(10/35)\nsm1.hats <- hatvalues(sm1)\nNmin[which(sm1.hats>0.5714286),]\n\n# Using the jackknife distance  \n\nX <- Nmin[,c(\"WEON\",\"WEOC\" ,\"WECN\",  \"SOM\", \"TSN\",\"TSC\",\"TSCN\",\"SolvitaCO2\",\"SHI\")] \nlibrary(bootstrap) # load package that has jackknife()\n## Write a function that returns the distance for each observation when it is not included in the dataset. Note the power and compactness of R's subsetting\ntheta<-function(x,xdata) \n{mahalanobis(xdata[-x,],colMeans(xdata[x,]), cov(xdata[x,]))}\njackD2<-jackknife(1:dim(X)[1], theta, X) #jackknife indicating to remove each of the whole set of rows in X\njackD2$jack.values  #compare with the Chi-square distribution\nqqPlot(jackD2$jack.values,distribution=\"chisq\", df=ncol(X))\nX[which(jackD2$jack.values==max(jackD2$jack.values)),] #maximum value\nqchisq(1-0.05/35,9) #gives me the critical chisquare value and I can check this effectively is an outlier\nX[which(jackD2$jack.values>qchisq(1-0.05/35,9)),] #outlier for real\nmax(jackD2$jack.values) #chi square value for the furthest observation (51)\n\n#Q6: Testing for collinearity\n\nvif(sm1) # values over 5 are undesirable and over 10 indicate excessive collinearity.\n\n#Q7 and Q8: Chosing a good model\n\ninstall.packages(\"leaps\")\n\nlibrary(leaps)\n\n\ninstall.packages(\"MASS\")\nlibrary(MASS) \n\n\napm1<-summary(best1<-regsubsets(NetNmin28~  WEON+WEOC +WECN + SOM +TSN +TSC  +TSCN+SolvitaCO2+SHI , Nmin, nbest=1, nvmax=9, method=c(\"exhaustive\")))\n\np<-c(1:9) # create a vector for p = number of parameters (columns 1 till 9 cause that's all the data)\n\nAICc<-apm1$bic - p*log(35) + 2*p + 2*p*(p+1)/(35-p-1) # calculate AICc # 35 is number of observations and p number of parameters\n\nplot(p,AICc) # plot the AIC vs. p and you look at the minimm one and for example if its 7 then you choose the one for 7 variables\n\ncoef(best1,which.min(AICc)) # get the coefficients for best model (negative signs affect negatively)\n\n#Q9:K-fold Crossvalidation\n\nsm2 <- lm(NetNmin28~  WEON+WEOC+TSC, Nmin)\nsummary(sm2)\nlibrary(DAAG)\n\ncv.lm(df=Nmin, sm2, m=3) # 4 fold cross-validation\n\nanova (sm1,sm2) # done to compare full model vs best model and expect to get ns values and thus I can use either of them and generally use the simplest one.\n\n\n",
    "created" : 1421190326303.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4123650340",
    "id" : "54DFB2FC",
    "lastKnownWriteTime" : 1421193333,
    "path" : "~/Google Drive/PLS206F13/RstudioPLS206/ISSPA 2015 regression script.R",
    "project_path" : "ISSPA 2015 regression script.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}